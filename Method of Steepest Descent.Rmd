---
title: "Method of Steepest Descent"
output: pdf_document
---



After factor screening, I ended up with 2 factors, preview length and match score,
that I had found to significantly influence browsing time.

I then wanted to roughly determine the optimal levels for these 2 factors.

I began with a $2^2$ factorial experiment with a center point condition. The
initial region of experimentation was the area in which my factor screening 
took place. Thus, the high level for preview length was 120 and low level for it
was 100. The high level for match score was 100 and low level was 80.

I first simulated the data for the center point and combined it with previously simulated 
data and then performed a curvature test in order to determine whether the initial region was already in the vicinity of
the optimum.

I found that although $\beta_{PQ}$ was significantly different
from 0, since its p-value was much lower than 0.01, its size was much larger relative to 
the other p-values. Thus, there was likely not a pure quadratic curvature in this area and so
the initial region was not likely to be in the vicinity of the optimum.






```{r echo = FALSE}

netflixffd1 <- read.csv("netflixffd.csv", header = TRUE)
netflixffd2 <- read.csv("netflixffd2.csv", header = TRUE)

netflix <- rbind(netflixffd1, netflixffd2)

phase2data = subset(netflix, select = -c(Prev.Type, Tile.Size))

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), rgb(8,48,107,maxColorValue = 255)))

# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}

center <- read.csv("center.csv", header = TRUE)

center = subset(center, select = -c(Prev.Type, Tile.Size))

phase2 <- rbind(phase2data, center)


# Function to create x and y grids for contour plots 
mesh <- function(x, y) { 
  Nx <- length(x)
  Ny <- length(y)
  list(
    x = matrix(nrow = Nx, ncol = Ny, data = x),
    y = matrix(nrow = Nx, ncol = Ny, data = y, byrow = TRUE)
  )
}

## Determine whether we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph1p <- data.frame(y = phase2$Browse.Time,
                  x1 = convert.N.to.C(U = phase2$Prev.Length, UH = 120, UL = 100),
                  x2 = convert.N.to.C(U = phase2$Match.Score, UH = 100, UL = 80))
ph1p$xPQ <- (ph1p$x1^2 + ph1p$x2^2)/2
## Check the average browsing time in each condition:
#aggregate(ph1p$y, by = list(x1 = ph1p$x1, x2 = ph1p$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
#mean(ph1p$y[ph1p$xPQ != 0]) - mean(ph1p$y[ph1p$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph1p)
#summary(m)


pvalues <- c("<2e-16","<2e-16","1.05e-07","<2e-16")

coefficients <- c("x1", "x2", "xPQ", "x1:x2")

df3 <- data.frame(coefficients,pvalues)
# head(df1)

#knitr::kable(df3, format = "markdown")


```


My next step was to use the method of steepest descent in order to determine
roughly where in the x-space the optimum was lying.

I first fitted the first order model to determine the direction of the path
of the steepest descent.
I also found the 2D contour plot, with the gradient on it and we see that the
starting point is (0,0).


```{r echo = FALSE}

mp.fo <- lm(y~x1+x2, data = ph1p)
beta0 <- coef(mp.fo)[1]
beta1 <- coef(mp.fo)[2]
beta2 <- coef(mp.fo)[3]
grdp <- mesh(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100))
x1 <- grdp$x
x2 <- grdp$y
etap.fo <- beta0 + beta1*x1 + beta2*x2

# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                convert.N.to.C(U = 120, UH = 120, UL = 100), 
                length.out = 100),
        y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                convert.N.to.C(U = 100, UH = 100, UL = 80), 
                length.out = 100), 
        z = etap.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Match Score",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=1)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)

## Calculate the coordinates along this path that we will experiment at

# The gradient vector
g <- matrix(c(beta1, beta2), nrow = 1)




# We will take steps of size 5 seconds in preview length. In coded units this is
PL.step <- convert.N.to.C(U = 110 + 5, UH = 120, UL = 100)
lamda <- PL.step/abs(beta1)


## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 120, UL = 100), 
                 Match.Score = round(convert.C.to.N(x = 0, UH = 100, UL = 80)))

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 6: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "6")
step6 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

## Step 7:
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "7")
step7 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))

pstdf.cond <- data.frame(Step = 0:7, rbind(step0, step1, step2, step3, step4, step5, step6, step7))
#pstdf.cond

```


Since preview length can only be changed in increments of 5, I decided that the
steps should be 5 seconds long in preview length. I then converted it from natural
units to coded units. I got that $\Delta x_1 = 0.5$ and so $\lambda = \frac{0.5}{\hat\beta_1}$ = 0.5661741.

I took one step at a time, until I found the lowest observed average browsing time,
and then I took a few more to make sure that I had found the lowest, since the 
following steps ended up with higher average browsing time.


```{r echo = FALSE}

## Load the data associated with the steepest descent search
ptsdf <- read.csv("ptsdf.csv", header = TRUE)
ptsdf = subset(ptsdf, select = -c(Prev.Type, Tile.Size))

phase3 <- rbind(ptsdf, center)


## Calculate the average browsing time in each of these conditions and find the 
## condition that minimizes it
pstdf.means <- aggregate(phase3$Browse.Time, 
                        by = list(Prev.Length = phase3$Prev.Length, 
                                  Match.Score = phase3$Match.Score), 
                        FUN = mean)

 plot(x = 0:7, y = rev(pstdf.means$x),
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
 points(x = 0:7, y = rev(pstdf.means$x),
       col = "red", pch = 16)

#pstdf.cond[pstdf.cond$Step == 4,]


p3final <- read.csv("p3.csv", header = TRUE)
ph2.5 <- data.frame(y = p3final$Browse.Time,
                  x1 = convert.N.to.C(U = p3final$Prev.Length, UH = 100, UL = 80),
                  x2 = convert.N.to.C(U = p3final$Match.Score, UH = 80, UL = 60))
ph2.5$xPQ <- (ph2.5$x1^2 + ph2.5$x2^2)/2

## Check the average browsing time in each condition:
#aggregate(ph2.5$y, by = list(x1 = ph2.5$x1, x2 = ph2.5$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
#mean(ph2.5$y[ph2.5$xPQ != 0]) - mean(ph2.5$y[ph2.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2.5)
#summary(m)

```


I found that step 4 corresponded to the lowest observed average browsing time.
It corresponded to a preview length of 90 seconds and match score of 70%.

In order to determine if I reached the vicinity of the optimum, I decided to perform
another test of curvature in this region. I ran another $2^2$ factorial experiment with
a center point and simulated the data. My new high level for preview length was 100, and low level was 80.
My new high level for match score was 80, and low level was 60.

This time, $\beta_{PQ}$ was significantly
different from 0 and the p-value for the pure quadratic was <2e-16 < 0.01, so I rejected
the null hypothesis that the pure quadratic was 0. Thus, I concluded that I was
in the vicinity of the optimum.
The optimal browsing time was somewhere in the vicinity of 90 seconds for
preview length, and 70% for match score.


