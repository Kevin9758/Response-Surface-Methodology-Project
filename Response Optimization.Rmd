---
title: "Response Optimization"
output: pdf_document
---




I decided to follow up my results with a response surface experiment so
that a full second order model could be fit and the optimum identified. I have
already identified the significant factors though factor screening,
and found a rough idea of where the optimum lies through the method of 
steepest descent.

The response surface experiment that I decided to use was the central composite
design. A central composite design facilitates estimation of the full second
order response surface model, and hence identification of the optimum.


```{r echo = FALSE}

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), rgb(8,48,107,maxColorValue = 255)))

# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}

# Function to create x and y grids for contour plots 
mesh <- function(x, y) { 
  Nx <- length(x)
  Ny <- length(y)
  list(
    x = matrix(nrow = Nx, ncol = Ny, data = x),
    y = matrix(nrow = Nx, ncol = Ny, data = y, byrow = TRUE)
  )
}


```


I chose the high and low levels of the factors based on the center point
being my estimate of the optimum from steepest descent in phase 2. I also
chose the high and low levels based on my selection of a spherical design in
order to ensure the estimate of the response surface at each condition is
equally precise.

```{r echo = FALSE}

ccdata <- read.csv("sphere.csv", header = TRUE)

condition <- data.frame(x1 = convert.C.to.N(x = c(-1,-1,1,1,0,1.5,-1.5,0,0), UH = 110, UL = 70), 
                        x2 = convert.C.to.N(x = c(-1,1,-1,1,0,0,0,1.5,-1.5), UH = 90, UL = 50))

pi_hat <- aggregate(x = ccdata$Browse.Time, by = list(condition.num = kronecker(1:9, rep(1, 100))), FUN = mean)
df3 <- data.frame(Condition.Num = pi_hat$condition.num, 
           Prev.Length = condition$x1, 
           Match.Score = condition$x2,
           Browse.Time = pi_hat$x)

knitr::kable(df3, format = "markdown")


```

I intended to perform axial conditions with $a$ = $\sqrt{2}$, but the corresponding
preview times and match scores were messy. Thus, in the interest of defining
experimental conditions with more convenient levels, I let $a$ = 1.5, yielding
the preview lengths and match scores in the table above. I then generated data
simulating 100 users randomized into each of these 9 conditions, and recorded
their browsing time.


```{r echo = FALSE}

par(mfrow = c(1, 2))

ph3 <- data.frame(y = ccdata$Browse.Time,
                  x1 = convert.N.to.C(U = ccdata$Prev.Length, UH = 110, UL = 70),
                  x2 = convert.N.to.C(U = ccdata$Match.Score, UH = 90, UL = 50))



## Check to see if that's significant

model <- lm(y ~ x1 + x2 + x1*x2 + 
              I(x1^2) + I(x2^2), data = ph3)

#summary(model)


beta0 <- coef(model)[1]
beta1 <- coef(model)[2]
beta2 <- coef(model)[3]
beta12 <- coef(model)[6]
beta11 <- coef(model)[4]
beta22 <- coef(model)[5]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 110, UL = 70), 
                    convert.N.to.C(U = 120, UH = 110, UL = 70), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 90, UL = 50), 
                    convert.N.to.C(U = 100, UH = 90, UL = 50), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.so <- beta0 + beta1*x1 + beta2*x2 + beta12*x1*x2 + beta11*x1^2 + beta22*x2^2

contour(x = seq(convert.N.to.C(U = 30, UH = 110, UL = 70), 
                convert.N.to.C(U = 120, UH = 110, UL = 70), 
                length.out = 100), 
        y = seq(convert.N.to.C(U = 0, UH = 90, UL = 50), 
                convert.N.to.C(U = 100, UH = 90, UL = 50), 
                length.out = 100), 
        z = eta.so, xlab = "x1", ylab = "x2",
        nlevels = 25, col = blue_palette(20), labcex = 0.9)

b <- matrix(c(beta1,beta2), ncol = 1)
B <- matrix(c(beta11, 0.5*beta12, 0.5*beta12, beta22), nrow = 2, ncol = 2)
x.s <- -0.5*solve(B) %*% b 
points(x = x.s[1], y = x.s[2], col = "red", pch = 16)


#x.s[1]
#x.s[2]




# The predicted browsing time at this configuration is:
eta.s <- beta0 + 0.5*t(x.s) %*% b
# eta.s

# convert.C.to.N(x = x.s[1,1], UH = 110, UL = 70)
# convert.C.to.N(x = x.s[2,1], UH = 90, UL = 50)

contour(x = seq(30, 120, length.out = 100), 
        y = seq(0, 100, length.out = 100), 
        z = eta.so, xlab = "Preview Length (seconds)", ylab = "Match Score",
        nlevels = 20, col = blue_palette(20), labcex = 0.9)

points(x = convert.C.to.N(x = x.s[1,1], UH = 110, UL = 70),
       y = convert.C.to.N(x = x.s[2,1], UH = 90, UL = 50), 
       col = "red", pch = 16)

points(x = 90, y = 70, pch = 16, col = "green")


```
I then fit the full second order response surface by fitting the second order
regression model.

From the coefficients in the output, I plotted the contour plot in coded units
and found that the stationary point was located at $x_1$ = -0.9266421 and 
$x_2$ = 0.3571865.

I then converted the contour plot to natural units. The corresponding stationary
point is when preview length is 71.47 (70) seconds and match score is 77.14% (77%),
represented by the red point. The green point is my rough estimate of the
optimal conditions from using the method of steepest descent in phase 2. It
suggested a preview length of 90 seconds and a match score of 70%.
As can be seen, it is somewhat close to the true optimum. 

The estimated browsing time at the optimum is 11.53 seconds and a 95% confidence interval is given by 
(11.3855,11.675).

Thus, Netflix should utilize preview lengths of 70 seconds and match scores of 77%
in order to minimize the browsing time by users.



